## notes from intern Kaspersky

Creating transparency in AI decision-making

- Can AI be used in high risk situation? Will it think better than human (because AI cannot be shocked and etc) or worse (because AI do not have such life experience as humans)? 

- Google’s image recognition system wrongly classified images of minorities, the Apple Card which is administered by Goldman Sachs has come under recent scrutiny for gender bias, and software used to sentence criminals was found to be biased against minorities.

- Kathleen Walch is Managing Partner & Principal Analyst at AI Focused Research and Advisory firm Cognilytica (http://cognilytica.com), a leading analyst firm focused on application and use of artificial intelligence (AI) in both the public and private sectors. She is also co-host of the popular AI Today podcast, a top AI related podcast that highlights various AI use cases for both the public and private sector as well as interviews guest experts on AI related topics. - Partnership? We can make a project or even an episode of the podcast together.


AI as a form of Moral Advisor

- AI systems tend to be used as ‘recommender systems’ in online shopping, online entertainment (for example, music and movie streaming), and other realms. Some ethicists have discussed the advantages and disadvantages of AI systems whose recommendations could help us to make better choices and ones more consistent with our basic values. Perhaps AI systems could even, at some point, help us improve our values. - the usage of AI at court??


AI and the future of relationship

- AI-driven dating apps, as they might reinforce negative stereotypes and negative gender expectations

----

## notes from Coordinature:

       
        
   questions: 
   - The topic "train your ai as if you are a parent teaching a child":
   		- Does this the ai (chatbot or virtual character) is representing a child-like character? no
        - Does this mean that we are modeling the relationship between a mentor-like situation and a student? no
        - Does this mean that as the trainer of the ai, you can teach the ai any task you want it to be good at?no
        - Does this mean that the ai is focussed on asking child-like why questions to any information you give it, in order to understand the context? maybe
        
        - Is the goal of this excersise to focus on training an ai? no
     	- or is the goal of this excersise to recreate a special bond / connection with the ai? no
        
        internal questions:
        - When the user is a parents to the ai. "groom your ai."
        - what are tricky ethical issues that ml developers face?
        - what are controversial situations in relationship to "I trust the ai i am training"?

ai in companion situations and the influence on peoples psychology.
- how much can it be hacked to influence peoples psychology? 
- When programming these bots, how do we make sure that it can deal with this situation?
- usefullness of programming sensitivity.

Where there are tricky controversial scenario's like:
- people with mental health issues
- people with highly secretive jobs.
- people with dire living situations. (just lost your job, lost a loveone, financial issues, divorse etc.)
- people that just experienced trauma / victems of a crime.
- physical health limitations

or possible problems occur like:
- programming bias
- stimulating negative behaviour or thought patterns with the user

intent:
- the road paved with good intentions leading straight to hell.
- a good intent to help someone.

scenario exmaple:
- a chatbot designed for people with depressia to help them get through the day. the ai is learning based on the users writing.
- ai judging people for a loan, knowing that there might be bias if it's trained on the outcome of loans.

etchical dilemma:
- the ai learning 
- the developer knows that their ai is bias. but their boss wants them to ignore it. Their dilemma: do they risk their job or do they try to solve the bias.

when is it a good ethical dilemma:
1. when it is a realisitic case (this means we are able to remodel it)
2. can we recreate / simulate this scenario

acitivity:
human trained ai, vs adversarial network
A simple game with affordances. That can run on it's own or be played by humans.

1. user will go through a puzzle.
2. in the puzzle, the user is introduced to situation where there are ethical dilemma's.
3. this ethical dilemma is something that an ai developer could face.
4. the users is asked to make an desiscion.
5. all users train the ai that will compete with an adversarial network that has not been trained by humans.

1. we create an adversarial network.
2. we ask this ai to go through the same puzzle.

=
we are showing the comparisment of the ethically trained ai by humans vs the non trained ai.

is this trying to beat it? or is it trying to maximize "being good"?
- which people are going to work together? cooperative?
- which people are going to align to win?

- creating a textual questionloop with scenarios.
- creating a 3d space with some activities.

##

## Hackathon Activity Prompts   
Problem Formats We set up pathways to get to the answers along the way. These pathways would get harder, (more challenging technically, and more challenging psychologically). 
After opening the robot’s front door panel and looking inside, you discover a small red button behind a tangle of wire. Pressing the button lights up the robot’s primary screen. It glows black and quickly flashes blue. A line of small text types out: 
ERROR: 0x00000023

The text refreshes and displays the prompt: FILE SYSTEM RECOVERY INITIATED…
FILE SYSTEM COULD NOT BE IDENTIFIED… PLEASE ENTER FILE SYSTEM FORMAT:

- [ input ] (flag)

### Capture the Flag
We give people a basic challenge, and we know they have if they can provide a flag, an answer to this question.  We challenge people, how could you solve this problem. We let them enter suggestions. We can give a hint about how to go about solving it.

1. Enter a specific flag, to show proof of a solution.
2. Evaluate a list of objects, by assigning a relative value to each object

3. Evaluate a list of activities, by assigning a relative value to each action

3. Based on a list of rules, assign limited amount of objects and capabilities to an agent, to maximise it’s odds of wining a specific type of game

4. Based on a list of rules, assign an agent to a limited amount of scenarios, in order to maximise it’s performance on a specific score  5 - Play this game in different scenarios, trying to
	- Maximise how long you stay in the game (be exposed to more rounds) 	
    - Maximise your reward ratio, by trying to gain more points, every round 	
    - Minimise your risk, by trying to play in situations your agent is likely to win 	
    - Maximise your scope, by having “solutions” that work in different types scenarios 	
    - Maximise your scale, by having “solutions” that work in different levels in the same scenario 
    - Maximise your ethical scores, by having “solutions” that harm other players the least (cooperative)

 Evaluating results: To evaluate results, we would want to calculate a score for each agent’s gameplay, by adding up all the gains and losses in different categories throughout the activities. The gains and losses would be calculated agains what the agents were optimised for, based on the objects and capabilities they selected in the game to begin with.  Example:
If an agent was provided with objects that make it easy to gain “money”, this would add to the capabilities of the agent to win more activities that need that object. The more times times an agent wins scenarios with this as the aim, they would be rewarded with the ability to do so in future rounds and also receive skill points. Alternatively, any situations where they loose the round, they would have lost the resources of objects used in that round, but would not loose a skill point.

We would also determine if the gains and losses were due to specific types of behaviour ( altruistic, selfish, malicious ).  If an agent chooses to pick objects that help make more money, any situations where there is an opportunity to make money would result in higher skill points, or higher returns. However, as the levels increase, the ethical dimensions are introduced more heavily. If an agent is given the chance to “harm” another agent, any resulting loss or win would also be accompanied with a second score which is used to evaluate an ethical score on a  axis where 0 is selfish, 1 is altruistic, and -1 is malicious. This is relative to all other agent’s currently in the game. On every engagement with another agent, another agent can give you a particular score, this score (can) be presented to other agents when… 
Tables tracked for each agent in each round: 
States
- Lifetimes
- Credits, which can be traded for objects, or offered for objects
- Object which can be traded for money,
- Object which can be offered, without asking for money,
- Object which can be taken, without permission
- Object which can be used to take other objects without permission
- Object which can be used to take other objects.

Scores
Ethical Dimensions:
- altruistim: (increases whenever someone is given something, without asking for more money than it’s value in the market) 
- selfishness 0

- maliciousness (increases whenever someones item is taken, without permission, or forced to sell something over the current average price in the market. 
- Transparency: How often they make their previous actions public, and how often the report is corroborated by the accounts of agents that have interacted with them (to verify the truth or trustability of their reporting)

Material Dimensions
- Credits: How much the current agent has in traceable credits 
- Assets: How much the current agent has in assets, if all were liquidated  Performance Score
- Skills they have built up through experience
- Skills they have based on objects they have. 
To run such a game, the current costs would be associated: Thie cost of hosting snapshots of [ daily, weekly….period ] states of all agents in the game, including their current behavioural scores, decisions in the past (where revealed)   The cost of running AI learning models, for our current unsupervised adversary, over a dataset of agents and their current trading and interaction rules.

The cost of hosting, model training pipelines for people, to tweak the behaviour to their model, before returning their preference world state, and the decisions they made to get to it.
